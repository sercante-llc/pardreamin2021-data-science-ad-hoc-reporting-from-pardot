{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53ea54d-3983-43c7-a7d5-5b0d2f41b270",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get email bounces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03149061-2307-42c6-9e22-9860c2d2210f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration\n",
    "We need some login details, we can pull in a config file or set some variables.\n",
    "https://towardsdatascience.com/keeping-credentials-safe-in-jupyter-notebooks-fbd215a8e311\n",
    "\n",
    "Provide a file name in \"config_file\" or provide values in the individual variables. The config file will override the listed variables.\n",
    "\n",
    "If we have a list of export IDs we wish to use, we can use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ffdd1-a487-42f7-aedc-0347fe527799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = {'pardot_api_creds':{}} # dictionary of configruation details, ignore this here\n",
    "\n",
    "#provide a value for \n",
    "\n",
    "config_file = \"pardotlogin.ipynb\" \n",
    "\n",
    "# OR all of the below\n",
    "\n",
    "cfg['pardot_api_creds']['PardotOath_Client_id'] = \"\"\n",
    "cfg['pardot_api_creds']['PardotOath_client_secret'] = \"\"\n",
    "cfg['pardot_api_creds']['PardotOath_username'] = \"\"\n",
    "cfg['pardot_api_creds']['PardotOath_password'] = \"\"\n",
    "cfg['pardot_api_creds']['PardotOath_token'] = \"\"\n",
    "\n",
    "cfg['pardot_api_creds']['PardotURL'] = \"\"\n",
    "cfg['pardot_api_creds']['apiversion'] = \"\"\n",
    "\n",
    "\n",
    "\n",
    "exportids=[] # list of ids to use, leave blank to autocreate  [1270,1272,1274]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab355839-4158-49b6-aec0-c17cc4c6defa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code Setup\n",
    "Making sure we have login details, getting a Pardot API library, logging in, and also setting some number formatting so our IDs read well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb59001-b090-4bfa-8309-9bc9d2dce4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the login details\n",
    "\n",
    "if config_file:\n",
    "    \"\"\"Ovrride any manually entered config above\"\"\"\n",
    "    %run $config_file\n",
    "\n",
    "\n",
    "# Pull in a Pardot API common library\n",
    "%run ./PardotAPI.ipynb\n",
    "\n",
    "\n",
    "#Log into Pardot\n",
    "access_token = PardotLogin()\n",
    "\n",
    "pd.options.display.float_format = '{:.0f}'.format # Get rid of adding trailing zeros to our numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc01eef-5b57-4319-8d61-9175110284e8",
   "metadata": {},
   "source": [
    "## Create an export of all the Prospects\n",
    "Because the export has a limit of 1 year of data per export, we get the Pardot account creation date and create an export file for every year the account has existed. Easy to forget to get all the years when a new year comes to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f11bb-ecd6-44c6-97a8-d6c2d5d69c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "PardotAccount = PardotAccountRead()\n",
    "\n",
    "if not exportids:\n",
    "    years = range(int(PardotAccount['account']['created_at'][ 0 : 4 ]),2021+1) #stops before the specified number\n",
    "    years_list = list(years)\n",
    "\n",
    "    for i in years_list:\n",
    "\n",
    "        url = cfg['pardot_api_creds']['PardotURL'] +\"/api/export/version/\"+ str(cfg['pardot_api_creds']['apiversion']) +\"/do/create?format=json\"\n",
    "\n",
    "        payload = json.dumps({\n",
    "          \"object\": \"Prospect\",\n",
    "          \"fields\": [\n",
    "            \"id\",\n",
    "            \"crm_lead_fid\",\n",
    "            \"crm_contact_fid\",\n",
    "            \"is_email_hard_bounced\",\n",
    "            \"email_bounced_at\",\n",
    "            \"email_bounced_reason\",\n",
    "            \"last_activity_at\",\n",
    "            \"score\",\n",
    "            \"first_assigned_at\",\n",
    "            \"crm_owner_fid\",\n",
    "            \"crm_last_sync\",\n",
    "            \"is_archived\",\n",
    "            \"updated_at\",\n",
    "          ],\n",
    "          \"procedure\": {\n",
    "            \"name\": \"filter_by_updated_at\",\n",
    "            \"arguments\": {\n",
    "              \"updated_after\": str(i)+\"-01-01 00:00:00\",\n",
    "              \"updated_before\": str(i)+\"-12-31 23:59:59\"\n",
    "            }\n",
    "          }\n",
    "        })\n",
    "        headers = {\n",
    "          'Content-Type': 'application/json',\n",
    "          'Authorization': 'Bearer ' + access_token,\n",
    "          'Pardot-Business-Unit-Id': cfg['pardot_api_creds']['PardotBusinesUnitID'],\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        response_dict = json.loads(response.text)\n",
    "        exportids.append(response_dict['export']['id'])\n",
    "\n",
    "        #print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c0e7a-336d-4671-bc03-e76873806304",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read the export\n",
    "Loop the export results and get data.\n",
    "Note that there is a number of commented out options. We often need to get different things, and it's easy to uncomment the thing that we want, and comment out those that we don't so we don't need to look eveyrthing up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3df295-4629-4d69-a15e-3c7d4dff7a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame() # Pandas Datafame library  - import pandas as pd \n",
    "\n",
    "#object = \"email_template_id\" # \"form_id\" \"email_template_id\"\n",
    "filelimit = None #1 # None or an int - This is used to limit impact while extending code so we aren't sucking in ALL the data everytime\n",
    "filecount = 0\n",
    "\n",
    "for exportid in exportids:\n",
    "    print (exportid)\n",
    "    FileURLs = PardotGetExportFilesURLList()    \n",
    "    #print(FileURLs)\n",
    "    for fileURL in FileURLs:\n",
    "        print(fileURL)\n",
    "        temp = PardotDownloadFileURL(fileURL)\n",
    "        #temp = temp[temp.email_template_id.notnull()]\n",
    "        #temp = temp[temp.email_id.notnull()]\n",
    "        \n",
    "        #temp = temp[temp[object].notnull()]\n",
    "        #temp = temp.filter(['details', 'campaign_id', object, 'email_id'])\n",
    "        #temp.drop_duplicates(subset =['details', object, \"campaign_id\"], inplace = True)     \n",
    "\n",
    "        #df = pd.concat([df, temp], axis=0)\n",
    "        #df.drop_duplicates(subset =['details', object, \"campaign_id\"], inplace = True) \n",
    "        \n",
    "        df = pd.concat([df, temp[temp['is_email_hard_bounced'].apply(str).str.startswith('1')]], axis=0)\n",
    "\n",
    "        \n",
    "        filecount +=1\n",
    "        if filelimit:\n",
    "            if filelimit <= filecount:\n",
    "                break\n",
    "    if filelimit:\n",
    "        if filelimit <= filecount:\n",
    "            break\n",
    "\n",
    "df.shape\n",
    "#temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0f766-b0a0-43a7-9469-b11332098179",
   "metadata": {},
   "source": [
    "## Seperate results into import ready files\n",
    "If we want to import these into the CRM or pardot, we want to prepare the files by record type for easier use with Dataloader or Pardot import with CRM ID matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f6fcb-fe46-4f0b-8206-f65752a8f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Contacts = df[df['crm_contact_fid'].apply(str).str.startswith('0')]\n",
    "Leads = df[df['crm_lead_fid'].apply(str).str.startswith('0')]\n",
    "Prospects = df[~df['crm_lead_fid'].apply(str).str.startswith('0')]\n",
    "Prospects = Prospects[~Prospects['crm_contact_fid'].apply(str).str.startswith('0')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea1a6e-0cf6-4300-a380-f1c557293526",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reports\n",
    "Some pretty graphs because we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bf584-82e8-428a-bd53-4f433b7aa1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = [ str(Prospects.count())+' Prospects', str(Leads.count())+' Leads', str(Contacts.count())+' Contacts']\n",
    "sizes = [Prospects.count(), Leads.count() , Contacts.count()]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "ax.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "ax.set_title('Bounced Emails by Record Types')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a006a9c-cd55-462f-8794-df390be514e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Safe file folder creation\n",
    "Create folders if we need to automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cece7da-9a53-4a51-a949-710d3fa4b3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Taken from https://stackoverflow.com/a/600612/119527\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc: # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else: raise\n",
    "\n",
    "def safe_open_w(path):\n",
    "    ''' Open \"path\" for writing, creating any parent directories as needed.\n",
    "    '''\n",
    "    mkdir_p(os.path.dirname(path))\n",
    "    return open(path, 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982eafca-d9f4-4ff6-b2ff-dbd4ab11bb76",
   "metadata": {},
   "source": [
    "## Write out data\n",
    "Write different fields for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2a808-cbef-46fa-a15f-1902b94382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = date.today().strftime( '%Y-%m-%d')\n",
    "AccountName = PardotAccount['account']['company'].replace(\" \", \"\")\n",
    "\n",
    "\n",
    "#you can change the row size according to your need\n",
    "#rowsize = 1048576 - 1 # 1048576 is as big as excel can cope with yet we need the header row\n",
    "rowsize = 1000000\n",
    "\n",
    "\n",
    "with safe_open_w(\"JupyterNotebookDataSave/\"+str(AccountName)+str(date.today())+\".csv\") as f:\n",
    "    df.to_csv(f, sep=\",\", float_format='%.2f', line_terminator='\\n',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727f43e-2560-40f2-9997-5db4f7daaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with safe_open_w(\"JupyterNotebookDataSave/\"+str(AccountName)+str(date.today())+\"Contacts.csv\") as f:\n",
    "    Contacts.to_csv(f, sep=\",\", float_format='%.2f', line_terminator='\\n',encoding='utf-8',index=False)\n",
    "    \n",
    "\n",
    "\n",
    "if len(Contacts) > rowsize:\n",
    "\n",
    "    #start looping through data writing it to a new file for each set\n",
    "\n",
    "    for i in range(1,len(Contacts),rowsize):\n",
    "        j = i+rowsize\n",
    "\n",
    "        #csv to write data to a new file with indexed name. input_1.csv etc.\n",
    "        out_csv = \"JupyterNotebookDataSave/\"+str(AccountName)+str(date.today())+\"Contacts[\" + str(i) + '].csv'\n",
    "        #out_csv = \"JupyterNotebookDataSave/Bring back Contacts from Recycle Bin[\" + str(i) + '].csv'\n",
    "\n",
    "        Contacts[i:j].to_csv(out_csv,\n",
    "              index=False,\n",
    "              #header=False,\n",
    "              #mode='a',#append data to csv file\n",
    "              )#size of data to append for each loop\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f61cc6-1def-4713-9c05-d6800c0ac829",
   "metadata": {},
   "outputs": [],
   "source": [
    "with safe_open_w(\"JupyterNotebookDataSave/\"+str(AccountName)+str(date.today())+\"Leads.csv\") as f:\n",
    "    Leads.to_csv(f, sep=\",\", float_format='%.2f', line_terminator='\\n',encoding='utf-8',index=False)\n",
    "  \n",
    "\n",
    "\n",
    "if len(Leads) > rowsize:\n",
    "\n",
    "    #start looping through data writing it to a new file for each set\n",
    "\n",
    "    for i in range(1,len(Leads),rowsize):\n",
    "        j = i+rowsize\n",
    "            \n",
    "        #csv to write data to a new file with indexed name. input_1.csv etc.\n",
    "        out_csv = \"JupyterNotebookDataSave/\"+str(AccountName)+str(date.today())+\"Leads[\" + str(i) + '].csv'\n",
    "        #out_csv = \"JupyterNotebookDataSave/Bring back Contacts from Recycle Bin[\" + str(i) + '].csv'\n",
    "\n",
    "        Leads[i:j].to_csv(out_csv,\n",
    "              index=False,\n",
    "              #header=False,\n",
    "              #mode='a',#append data to csv file\n",
    "              )#size of data to append for each loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ffa11-e795-4828-8ed7-c363c0d1cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with safe_open_w(\"output/\"+str(AccountName)+str(date.today())+\"Prospects.csv\") as f:\n",
    "    Prospects.to_csv(f, sep=\",\", float_format='%.2f', line_terminator='\\n',encoding='utf-8',index=False)\n",
    "        \n",
    "\n",
    "if len(Prospects) > rowsize:\n",
    "\n",
    "    #start looping through data writing it to a new file for each set\n",
    "\n",
    "    for i in range(1,len(Prospects),rowsize):\n",
    "        j = i+rowsize\n",
    "\n",
    "        #csv to write data to a new file with indexed name. input_1.csv etc.\n",
    "        out_csv = \"output/\"+str(AccountName)+str(date.today())+\"Prospects[\" + str(i) + '].csv'\n",
    "        #out_csv = \"JupyterNotebookDataSave/Bring back Contacts from Recycle Bin[\" + str(i) + '].csv'\n",
    "\n",
    "        Prospects[i:j].to_csv(out_csv,\n",
    "              index=False,\n",
    "              #header=False,\n",
    "              #mode='a',#append data to csv file\n",
    "              chunksize=rowsize)#size of data to append for each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c393a-e87d-4a2f-a9d3-17ae26a51098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete - Records: \",len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
